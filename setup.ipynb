{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setting up the data.\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the size of data', (6000,))\n"
     ]
    }
   ],
   "source": [
    "data = np.load('data/strokes.npy')\n",
    "print('the size of data',data.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the data has mean', (0.45000002, 0.04828767), 'and std', (0.6908155, 0.5390064))\n"
     ]
    }
   ],
   "source": [
    "data_x = []\n",
    "data_y = []\n",
    "for i in range(len(data)):\n",
    "    cur_data = data[i]\n",
    "    for j in range(cur_data.shape[0]):\n",
    "        if cur_data[j,0] == 1:\n",
    "            data_x.append(cur_data[j,1])\n",
    "            data_y.append(cur_data[j,2])\n",
    "data_means = (np.mean(data_x),np.mean(data_y))\n",
    "data_stds = (np.std(data_x),np.std(data_y))\n",
    "print('the data has mean',data_means, 'and std',data_stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('max', 1191, 'min', 301, 'mean', 644.2076666666667)\n",
      "(2753, ' are > mean')\n"
     ]
    }
   ],
   "source": [
    "# max sequence size\n",
    "seq_len = []\n",
    "for i in range(len(data)):\n",
    "    seq_len.append(data[i].shape[0])\n",
    "print('max', max(seq_len),'min',min(seq_len),'mean',np.mean(seq_len))\n",
    "mean = np.mean(seq_len)\n",
    "seq_skew = []\n",
    "for i in range(len(data)):\n",
    "    seq_skew.append(data[i].shape[0]>mean)\n",
    "print(sum(seq_skew),' are > mean')\n",
    "# not very skewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('max', 96.15)\n",
      "0.039828254139581284\n"
     ]
    }
   ],
   "source": [
    "# max action size\n",
    "\n",
    "seq_len = []\n",
    "for i in range(len(data)):\n",
    "    seq_len.append(np.max(data[i]))\n",
    "print('max', max(seq_len))\n",
    "\n",
    "# check ratio of pen up and down:\n",
    "\n",
    "seq_len = np.zeros((0))\n",
    "for i in range(len(data)):\n",
    "    seq_len = np.concatenate([seq_len,data[i][:,0]],0)\n",
    "print(np.mean(seq_len))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'epoch_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m--------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-b3269cb77197>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_limit\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata_start\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_end\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_loc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_loc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-b3269cb77197>\u001b[0m in \u001b[0;36mdataloader\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;31m#print(total)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtotal\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepoch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepoch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m         \u001b[0mcur_data_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[1;32mwhile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcur_data_iter\u001b[0m\u001b[1;33m<\u001b[0m\u001b[0mepoch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'module' object has no attribute 'epoch_size'"
     ]
    }
   ],
   "source": [
    "# dataset_script\n",
    "\n",
    "# with only down strokes \n",
    "# ('the data has mean', (0.0422648, 0.56823623), 'and std', (0.20119265, 1.120318))\n",
    "# with all strokes:\n",
    "# ('the data has mean', (0.039828256, 0.41248125), 'and std', (0.19555554, 2.0786476))\n",
    "# In the paper, the data is normalized first.\n",
    "import global_variables as gv\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class dataloader():\n",
    "    \n",
    "    def __init__(self,batch_limit,data_start, data_end, data_loc=gv.data_loc):\n",
    "        self.all_data = np.load(data_loc)\n",
    "        self.all_data = [self.all_data[i] for i in range(data_start,data_end)]\n",
    "        self.means = gv.means\n",
    "        self.stds = gv.stds\n",
    "        self.shuffled_index = range(len(self.all_data))\n",
    "        random.shuffle(self.shuffled_index)\n",
    "        self.all_data = [self.all_data[i] for i in self.shuffled_index]\n",
    "        self.batch_limit = batch_limit\n",
    "        \n",
    "    def shuffle_index():\n",
    "        random.shuffle(self.shuffled_index)\n",
    "        self.all_data = [self.all_data[i] for i in self.shuffled_index]\n",
    "        \n",
    "    def get_next_count(self,data,new_ar):\n",
    "        new_data = [x for x in data]\n",
    "        new_data.append(new_ar)\n",
    "        size = [len(x) for x in new_data]\n",
    "        total = max(size)*len(new_data)\n",
    "        #print(total)\n",
    "        return total\n",
    "    def get_data(self,epoch_size=gv.epoch_size):\n",
    "        cur_data_iter = 0\n",
    "        while(cur_data_iter<epoch_size):\n",
    "#             'returned here')\n",
    "            data = []\n",
    "            count = 0\n",
    "            while(self.get_next_count(data,self.all_data[cur_data_iter])<self.batch_limit):\n",
    "                data.append(self.all_data[cur_data_iter])\n",
    "                cur_data_iter +=1\n",
    "                if cur_data_iter>=epoch_size: break\n",
    "            data_inp = [np.concatenate([np.zeros((1,3)), x[:-1]],0) for x in data]\n",
    "            yield data_inp,data\n",
    "        \n",
    "\n",
    "            \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = dataloader(4000,0,5500)\n",
    "data_iter = x.get_data()\n",
    "data_inp,data_gt = next(data_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "cat_target = np.concatenate(data_gt,axis=0)\n",
    "x_gt = torch.autograd.Variable(torch.FloatTensor(cat_target[:,1]))\n",
    "y_gt = torch.autograd.Variable(torch.FloatTensor(cat_target[:,2]))\n",
    "pen_down_gt = torch.autograd.Variable(torch.FloatTensor(cat_target[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "seq_lengths = map(len, data)\n",
    "limit = max(seq_lengths)\n",
    "seq_tensor = Variable(torch.zeros((limit,len(data),3)))\n",
    "sorted_ind = np.argsort(seq_lengths)[::-1]\n",
    "for idx, (sample, seqlen) in enumerate(zip(data, seq_lengths)):\n",
    "    seq_tensor[:seqlen, sorted_ind[idx], :] = torch.FloatTensor(sample)\n",
    "    \n",
    "seq_lengths.sort(reverse=True)\n",
    "# seq_lengths, perm_idx = seq_lengths.sort(0, descending=True)\n",
    "# seq_tensor = seq_tensor[perm_idx]\n",
    "pack = torch.nn.utils.rnn.pack_padded_sequence(seq_tensor,seq_lengths)\n",
    "hx = torch.autograd.Variable(torch.zeros(1,len(data), 300))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "network = simple_GRU(3, 300,121)\n",
    "output = network.gru_1(pack, hx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unpacked, unpacked_len = torch.nn.utils.rnn.pad_packed_sequence(output[0])\n",
    "unsort_pattrn  = np.argsort(sorted_ind)\n",
    "inp_linear = []\n",
    "for j in unsort_pattrn:\n",
    "    inp_linear.append(unpacked[:unpacked_len[j],j,:])\n",
    "#print(inp_linear[0].size(),inp_linear[1].size())\n",
    "inp_linear = torch.cat(inp_linear,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3264, 121])\n"
     ]
    }
   ],
   "source": [
    "output = network.linear(inp_linear)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mixture_coef(output):\n",
    "    z = output # pen states\n",
    "    z_pi, z_mu1, z_mu2, z_sigma1, z_sigma2, z_corr = torch.split(z[:, :-1], 20, 1)\n",
    "    pen_down_prob = torch.sigmoid(z[:,2])\n",
    "\n",
    "    # process output z's into MDN paramters\n",
    "    # softmax all the pi's and pen states:\n",
    "    z_pi = torch.nn.functional.softmax(z_pi)\n",
    "    # all our actions\n",
    "    z_mu1 =  2/math.pi*40*torch.atan(z_mu1)\n",
    "    z_mu2 =  2/math.pi*40*torch.atan(z_mu2)\n",
    "\n",
    "    # exponentiate the sigmas and also make corr between -1 and 1.\n",
    "    z_sigma1 = torch.exp(z_sigma1)\n",
    "    z_sigma2 = torch.exp(z_sigma2)\n",
    "    z_corr = 0.99* 2/math.pi*torch.atan(z_corr)\n",
    "\n",
    "    r = [pen_down_prob,z_pi, z_mu1, z_mu2, z_sigma1, z_sigma2, z_corr]\n",
    "    return r\n",
    "\n",
    "def normal_2d_pdfval(x1, x2, mu1, mu2, s1, s2, rho):\n",
    "    norm1 = x1- mu1\n",
    "    norm2 = x2- mu2\n",
    "    s1s2 = s1*s2\n",
    "    # eq 25\n",
    "    z = torch.pow(norm1/s1,2) + torch.pow(norm2/s2,2) - 2 * rho*norm1*norm2/s1s2\n",
    "    neg_rho = 1 - torch.pow(rho,2)\n",
    "    result = torch.exp(-z/(2 * neg_rho))\n",
    "    denom = 2 * np.pi * s1s2* torch.sqrt(neg_rho)+1e-20\n",
    "    #print(np.unique(np.sign(result.cpu().data.numpy())),'result')\n",
    "    #print(np.unique(np.sign(denom.cpu().data.numpy())),'denom')\n",
    "    result = result/denom\n",
    "    # print(result.size())\n",
    "    return result\n",
    "\n",
    "def loss_distr(pen_down_prob, z_pi, z_mu1, z_mu2, z_sigma1, z_sigma2, z_corr,x1_data, x2_data,pen_data):\n",
    "    \"\"\"Returns a loss fn based on eq #26 of http://arxiv.org/abs/1308.0850.\"\"\"\n",
    "    # This represents the L_R only (i.e. does not include the KL loss term).\n",
    "    x1_data = torch.stack((x1_data,)*20,1)\n",
    "    x2_data = torch.stack((x2_data,)*20,1)\n",
    "    \n",
    "    result0 = normal_2d_pdfval(x1_data, x2_data, z_mu1, z_mu2, z_sigma1, z_sigma2,z_corr)\n",
    "    #print('result 0',np.unique(np.sign(result0.cpu().data.numpy())))\n",
    "    epsilon = 1e-6\n",
    "    #print(np.unique(z_pi.data.cpu().numpy()>=0))\n",
    "    # result1 is the loss wrt pen offset (L_s in equation 9 of\n",
    "    # https://arxiv.org/pdf/1704.03477.pdf)\n",
    "    result1 = result0* z_pi\n",
    "    result1 = torch.sum(result1, 1) + epsilon\n",
    "    result1 = -torch.log(result1)  # avoid log(0)\n",
    "\n",
    "    #fs = 1.0 - pen_data[:, 2]  # use training data for this\n",
    "    #fs = torch.reshape(fs, [-1, 1])\n",
    "    # Zero out loss terms beyond N_s, the last actual stroke\n",
    "    #result1 = result1* fs\n",
    "\n",
    "    # result2: loss wrt pen state, (L_p in equation 9)\n",
    "    # modify pendata to be 2 channel\n",
    "    # add softmax loss\n",
    "    result2 = softmax_loss(\n",
    "             labels=pen_data, logits=pen_down_prob)\n",
    "    #result2 = tf.reshape(result2, [-1, 1])\n",
    "    #if not self.hps.is_training:  # eval mode, mask eos columns\n",
    "    #    result2 = result2* fs\n",
    "    result = torch.mean(result1) #+ result2\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 3.2722\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "pen_down_prob,o_pi, o_mu1, o_mu2, o_sigma1, o_sigma2, o_corr = get_mixture_coef(output)\n",
    "loss_distr = loss_distr(o_pi, o_mu1, o_mu2, o_sigma1, o_sigma2, o_corr, x_gt, y_gt)\n",
    "loss_sigmoid =  torch.nn.BCELoss()\n",
    "# print(type(pen_down_prob.data),type(pen_down_gt.data))\n",
    "loss = loss_sigmoid(pen_down_prob,pen_down_gt)\n",
    "loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "optimizer = torch.optim.Adam(network.parameters(), 0.001,weight_decay=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# models script\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import torch.nn as nn\n",
    "# import tensorboardX as tb\n",
    "\n",
    "class simple_GRU(nn.Module):\n",
    "    \n",
    "    def __init__(self, inp_size, hidden_size_1,output_size):\n",
    "        super(simple_GRU, self).__init__()\n",
    "        self.gru_1 = nn.GRU(inp_size, hidden_size_1)\n",
    "        self.linear = nn.Linear(hidden_size_1,output_size)\n",
    "\n",
    "        # for training\n",
    "        self.drop = nn.Dropout(0.4)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self,input, hidden, train=True):\n",
    "        # assuming tensor input k*3\n",
    "        ln, hn = self.gru_1(input, hidden)\n",
    "        rearranged = ln.squeeze(1)\n",
    "        if train: rearranged = self.drop(rearranged)\n",
    "        out1 = self.linear(self.relu(rearranged))\n",
    "        return out1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import data_loader\n",
    "import utils\n",
    "import global_variables as gv\n",
    "import models\n",
    "import argparse\n",
    "# import tensorboardX\n",
    "import numpy as np\n",
    "\n",
    "def setup(cuda,device_id=0):\n",
    "    if cuda:\n",
    "        torch.cuda.set_device(device_id)\n",
    "    train_data_loader = data_loader.dataloader(gv.batch_limit,gv.train_start_index,gv.train_end_index)\n",
    "    val_data_loader = data_loader.dataloader(gv.batch_limit,gv.val_start_index,gv.val_end_index)\n",
    "    network = models.simple_GRU(3,gv.gru_size,121)\n",
    "    network.cuda()\n",
    "    # init the network with orthogonal init and gluroot.\n",
    "    # network.wt_init()\n",
    "    \n",
    "    graves_output = models.graves_output()\n",
    "    \n",
    "    # Optimizer \n",
    "    optimizer = torch.optim.Adam(network.parameters(), gv.orig_lr, weight_decay=gv.weight_decay)\n",
    "    \n",
    "    # for le rumours \n",
    "    cudnn.benchmark = True\n",
    "    \n",
    "    return train_data_loader, val_data_loader, network, graves_output, optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'models' from 'models.pyc'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def train(train_data_loader, network, graves_output,optimizer):\n",
    "    \n",
    "    data_fetcher = train_data_loader.get_data()\n",
    "    for iter,(data,data_gt) in enumerate(data_fetcher):\n",
    "        if iter%100 == 0 : print('cur_iter',iter)\n",
    "        cat_target = np.concatenate(data_gt,axis=0)\n",
    "        x_gt = torch.autograd.Variable(torch.FloatTensor(cat_target[:,1]))\n",
    "        y_gt = torch.autograd.Variable(torch.FloatTensor(cat_target[:,2]))\n",
    "        pen_down_gt = torch.autograd.Variable(torch.FloatTensor(cat_target[:,0]))\n",
    "        output = network.forward_unlooped(data,cuda=True)\n",
    "        pen_down_prob,o_pi, o_mu1, o_mu2, o_sigma1, o_sigma2, o_corr = graves_output.get_mixture_coef(output)\n",
    "        loss_distr = graves_output.loss_distr(o_pi, o_mu1, o_mu2, o_sigma1, o_sigma2, o_corr, x_gt, y_gt)\n",
    "        \n",
    "        \n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss_distr.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "def val(val_data_loader,network,graves_output):\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_loader, val_data_loader, network, graves_output,optimizer = setup(True,0)\n",
    "for epoch in range(gv.total_epochs):\n",
    "    utils.adjust_learning_rate(optimizer, epoch,gv.orig_lr)\n",
    "    train_data_loader.shuffle_index()\n",
    "    train(train_data_loader, network, graves_output,optimizer)\n",
    "    print('==========TRAIN Epoch',iter+1,\"COMPLETE ====================\")\n",
    "    loss = val(val_data_loader,network,graves_output)\n",
    "    print('==========val Epoch',iter+1,\"COMPLETE ====================\")\n",
    "    # add a is best checker\n",
    "    utils.save_checkpoint({\n",
    "           'epoch': epoch + 1,\n",
    "           'arch': 'res18',\n",
    "           'loss': loss,\n",
    "           'model_state_dict': network.state_dict(),\n",
    "           'optimizer' : optimizer.state_dict(),\n",
    "        },filename = 'weights/simple_GRU_'+str(epoch+1)+'.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_stroke_numpy(stroke, save_name=None):\n",
    "    # Plot a single example.\n",
    "    f, ax = pyplot.subplots()\n",
    "    x = np.cumsum(stroke[:, 1])\n",
    "    y = np.cumsum(stroke[:, 2])\n",
    "\n",
    "    size_x = x.max() - x.min() + 1.\n",
    "    size_y = y.max() - y.min() + 1.\n",
    "\n",
    "    f.set_size_inches(5. * size_x / size_y, 5.)\n",
    "\n",
    "    cuts = np.where(stroke[:, 0] == 1)[0]\n",
    "    start = 0\n",
    "\n",
    "    for cut_value in cuts:\n",
    "        ax.plot(x[start:cut_value], y[start:cut_value],\n",
    "                'k-', linewidth=3)\n",
    "        start = cut_value + 1\n",
    "    ax.axis('equal')\n",
    "    ax.axes.get_xaxis().set_visible(False)\n",
    "    ax.axes.get_yaxis().set_visible(False)\n",
    "    \n",
    "    f.tight_layout(pad=0)\n",
    "    f.canvas.draw()\n",
    "    data = np.fromstring(f.canvas.tostring_rgb(), dtype=np.uint8, sep='')\n",
    "    data = data.reshape(f.canvas.get_width_height()[::-1]+(3,))\n",
    "    pyplot.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fea678f0f90>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3AAAACdCAYAAAD12JoWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fea580e1190>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pyplot.figure(figsize=(15,7))\n",
    "pyplot.imshow(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(739, 3)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:36: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(500, 3433, 3)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat = plot_stroke(data[0])\n",
    "dat.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
